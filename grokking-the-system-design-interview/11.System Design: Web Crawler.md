# System Design: Web Crawler

Learn about the web crawler service.

## We'll cover the following:

- Introduction
- Additional benefits
- How will we design a Web crawler?

## Introduction

A web crawler is an Internet bot that systematically scours the World Wide Web (WWW) for content, starting its operation from a pool of seed URLs. This process of acquiring content from the WWW is called **crawling**. The content collected during the crawl is stored in data stores, a process known as **storing**.

This is the first step performed by search engines; the stored data is used for indexing and ranking purposes. However, this specific design problem focuses only on web crawlers and does not cover the later stages of indexing and ranking in search engines.

## An Overview of the Web Crawler System

## Additional Benefits

The additional utilities of a web crawler are as follows:

- **Web page testing**: Used to check the validity of the links and structure of web pages.
- **Web page monitoring**: Used to monitor content or structural updates on web pages.
- **Site mirroring**: Efficiently mirrors popular websites.
- **Copyright infringement check**: Fetches content to check for copyright infringement issues.

In this chapter, we’ll design a web crawler and evaluate how it fulfills both functional and non-functional requirements.

The output of the crawling process is the data that serves as input for subsequent processing phases, such as data cleaning, indexing, and page relevance algorithms (e.g., PageRank). For details on these later stages, refer to our chapter on **distributed search**.

## How Will We Design a Web Crawler?

This chapter consists of four lessons that cover the overall design of the web crawler system:

### 1. **Requirements**

- This lesson lists the functional and non-functional requirements of the system.
- Estimates calculations for various system parameters.

### 2. **Design**

- Analyzes a bottom-up approach for a web crawling service.
- Provides a detailed overview of all individual components leading to a combined operational mechanism to meet the system requirements.

### 3. **Improvements**

- Provides design improvements required to handle challenges such as crawler traps.
- Examples of crawler traps include:
  - Links with query parameters
  - Internal link redirection
  - Infinite calendar pages
  - Links generating dynamic content
  - Cyclic directories

### 4. **Evaluation**

- Evaluates design choices to ensure they meet expected standards and requirements.

Let’s begin by defining the requirements of a web crawler.
